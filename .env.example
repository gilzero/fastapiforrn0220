# Server Configuration
PORT=3050

# API Keys
OPENAI_API_KEY=
ANTHROPIC_API_KEY=
GEMINI_API_KEY=
GROQ_API_KEY=

# SUPPORTED PROVIDERS
SUPPORTED_PROVIDERS="gpt,claude,gemini"

# Model
OPENAI_MODEL_DEFAULT="gpt-4o"
OPENAI_MODEL_FALLBACK="gpt-4o-mini"
ANTHROPIC_MODEL_DEFAULT="claude-3-5-sonnet-latest"
ANTHROPIC_MODEL_FALLBACK="claude-3-5-haiku-latest"
GEMINI_MODEL_DEFAULT="gemini-2.0-flash"
GEMINI_MODEL_FALLBACK="gemini-1.5-pro"
GROQ_MODEL_DEFAULT="llama-3.3-70b-versatile"
GROQ_MODEL_FALLBACK="mixtral-8x7b-32768"

# Temperature
OPENAI_TEMPERATURE=0.3
ANTHROPIC_TEMPERATURE=0.3
GEMINI_TEMPERATURE=0.3
GROQ_TEMPERATURE=0.3

# MAX TOKENS
OPENAI_MAX_TOKENS=8192
ANTHROPIC_MAX_TOKENS=8192
GEMINI_MAX_TOKENS=8192
GROQ_MAX_TOKENS=8192

# Message Validation
# MAX_MESSAGE_LENGTH: Maximum characters per message (approximately 6000 tokens)
# MAX_MESSAGES_IN_CONTEXT: Maximum total messages in conversation (including both user & AI messages)
# MIN_MESSAGE_LENGTH: Minimum characters required for a message
MAX_MESSAGE_LENGTH=24000
MAX_MESSAGES_IN_CONTEXT=50
MIN_MESSAGE_LENGTH=1

# Rate Limiting Configuration
# 200 requests per hour (~3.3 requests per minute) for all providers

RATE_LIMIT_MAX_REQUESTS=500
RATE_LIMIT_WINDOW_SECONDS=3600

# Test Configuration
ENABLE_FULL_RATE_LIMIT_TEST=false  # Set to true to run full rate limit test

# Logging Configuration
LOG_LEVEL=info
LOG_FILE_PATH=logs/app.log

# Environment
PYSERVER_ENV=development

# Response timeout in seconds
RESPONSE_TIMEOUT=30.0