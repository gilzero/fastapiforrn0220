# Server Configuration
PORT=3050

# API Keys
OPENAI_API_KEY=
ANTHROPIC_API_KEY=
GEMINI_API_KEY=
GROQ_API_KEY=

# Model
OPENAI_MODEL_DEFAULT="gpt-4o"
OPENAI_MODEL_FALLBACK="gpt-4o-mini"
ANTHROPIC_MODEL_DEFAULT="claude-3-5-sonnet-latest"
ANTHROPIC_MODEL_FALLBACK="claude-3-5-haiku-latest"
GEMINI_MODEL_DEFAULT="gemini-2.0-flash"
GEMINI_MODEL_FALLBACK="gemini-1.5-pro"
GROQ_MODEL_DEFAULT="llama-3.3-70b-versatile"
GROQ_MODEL_FALLBACK="mixtral-8x7b-32768"

# Temperature
OPENAI_TEMPERATURE=0.3
ANTHROPIC_TEMPERATURE=0.3
GEMINI_TEMPERATURE=0.3
GROQ_TEMPERATURE=0.3

# MAX TOKENS
OPENAI_MAX_TOKENS=8192
ANTHROPIC_MAX_TOKENS=8192
GEMINI_MAX_TOKENS=8192
GROQ_MAX_TOKENS=8192

# Message Validation
# MAX_MESSAGE_LENGTH: Maximum characters per message (approximately 6000 tokens)
# MAX_MESSAGES_IN_CONTEXT: Maximum total messages in conversation (including both user & AI messages)
# MIN_MESSAGE_LENGTH: Minimum characters required for a message
MAX_MESSAGE_LENGTH=24000
MAX_MESSAGES_IN_CONTEXT=50
MIN_MESSAGE_LENGTH=1

# Rate Limiting
# 1 hour in milliseconds
# 500 total requests per hour (~8.3 requests per minute)
# 200 GPT requests per hour (~3.3 per minute)
# 200 Claude requests per hour (~3.3 per minute)
# 200 Gemini requests per hour (~3.3 per minute)
# 200 Groq requests per hour (~3.3 per minute)

RATE_LIMIT_WINDOW_MS=3600000
RATE_LIMIT_MAX_REQUESTS=500
GPT_RATE_LIMIT_MAX=200
CLAUDE_RATE_LIMIT_MAX=200
GEMINI_RATE_LIMIT_MAX=200
GROQ_RATE_LIMIT_MAX=200

# Logging Configuration
LOG_LEVEL=info
LOG_FILE_PATH=logs/app.log

# Environment
PYSERVER_ENV=development